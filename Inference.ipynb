{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87851c1e-86fc-4179-8ba1-614bfa31c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import argparse\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d61fb5-2787-4ecb-8ccb-a0615422f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports from your project structure\n",
    "from model.gru_audio_model import RNN, GRUAudioConfig\n",
    "from audioDataLoader.mulaw import mu_law_encode, mu_law_decode\n",
    "\n",
    "from utils.utils import multi_linspace, steps, plot_condition_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2eae26-053c-4b32-ba24-4825fa085ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_directory = \"./output/20250805_162729\" #'Path to the directory of the saved run.'\n",
    "top_n = 5 #'Sample from the top N most likely outputs.'\n",
    "temperature =1.0 #'Controls the randomness of predictions.'\n",
    "length_seconds =2.0 #'Length of the audio to generate in seconds.'\n",
    "\n",
    "sample_rate = 16000\n",
    "generation_length = int(length_seconds * sample_rate)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#parser.add_argument('--output_wav_path', type=str, default='generated_audio.wav', help='Path to save the output WAV file.')\n",
    "#parser.add_argument('--output_plot_path', type=str, default='generated_waveform.png', help='Path to save the output plot.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9df5d7-a54b-40a9-bed3-13e2ce9a9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------     Load model     -----------#\n",
    "\n",
    "config_path = os.path.join(run_directory, \"config.pt\")\n",
    "checkpoint_path = os.path.join(run_directory, \"checkpoints\", \"last_checkpoint.pt\")\n",
    "\n",
    "assert os.path.exists(run_directory), f\"Run directory not found: {run_directory}\"\n",
    "assert os.path.exists(config_path), f\"Config file not found: {config_path}\"\n",
    "assert os.path.exists(checkpoint_path), f\"Checkpoint file not found: {checkpoint_path}\"\n",
    "\n",
    "saved_configs = torch.load(config_path, weights_only=False)\n",
    "model_config = saved_configs[\"model_config\"]\n",
    "\n",
    "model = RNN(model_config).to(device)\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model successfully loaded from checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe8ad82-819f-4c81-b373-a924cbc58eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, cond_seq, warmup_sequence, top_n=3, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generates audio sequence based on a conditioning sequence.\n",
    "\n",
    "    Args:\n",
    "        model: The trained RNN model.\n",
    "        cond_seq (torch.Tensor): The sequence of conditioning parameters. Shape: (seq_len, num_cond_params).\n",
    "        warmup_sequence (torch.Tensor): A raw audio sequence to warm up the model's hidden state. Shape: (warmup_len,).\n",
    "        top_n (int): The number of top predictions to sample from.\n",
    "        temperature (float): Controls the randomness of predictions. Higher is more random.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The generated audio waveform.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    print(\"Starting inference...\")\n",
    "\n",
    "    # --- 1. Warm-up Phase --- \n",
    "    print(\"Warming up model hidden state...\")\n",
    "    warmup_encoded = mu_law_encode(warmup_sequence, quantization_channels=256)\n",
    "    warmup_input_audio = (warmup_encoded.float() / 255.0).to(device)\n",
    "\n",
    "    first_cond_vec = cond_seq[0].unsqueeze(0).repeat(len(warmup_input_audio), 1).to(device)\n",
    "    warmup_full_input = torch.cat([warmup_input_audio.unsqueeze(-1), first_cond_vec], dim=-1)\n",
    "\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    for i in range(len(warmup_full_input)):\n",
    "        _, hidden = model(warmup_full_input[i].unsqueeze(0), hidden, batch_size=1)\n",
    "\n",
    "    next_input_audio = warmup_input_audio[-1].unsqueeze(0)\n",
    "\n",
    "    # --- 2. Generation Phase --- \n",
    "    print(f\"Generating {len(cond_seq)} audio samples...\")\n",
    "    generated_samples = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(cond_seq)):\n",
    "            current_cond_vec = cond_seq[i].unsqueeze(0).to(device)\n",
    "            next_input_full = torch.cat([next_input_audio.unsqueeze(-1), current_cond_vec], dim=-1)\n",
    "\n",
    "            logits, hidden = model(next_input_full, hidden, batch_size=1)\n",
    "\n",
    "            logits = logits.div(temperature).squeeze()\n",
    "            top_n_logits, top_n_indices = torch.topk(logits, top_n)\n",
    "            top_n_probs = F.softmax(top_n_logits, dim=-1)\n",
    "            sampled_relative_idx = torch.multinomial(top_n_probs, 1).squeeze()\n",
    "            sampled_mu_law_index = top_n_indices[sampled_relative_idx]\n",
    "\n",
    "            new_audio_sample = mu_law_decode(sampled_mu_law_index, quantization_channels=256)\n",
    "            generated_samples.append(new_audio_sample.item())\n",
    "\n",
    "            next_input_audio = (mu_law_encode(new_audio_sample.unsqueeze(0), 256).float() / 255.0).to(device)\n",
    "\n",
    "    print(\"Inference complete.\")\n",
    "    return np.array(generated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1daee6c-df6b-4461-9855-f4565bbfd34d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d0570-d3a7-4abb-b895-c989c613f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_cond_params = model_config.cond_size\n",
    "cond_seq = torch.zeros(generation_length, num_cond_params)\n",
    "cond_seq[:, 0] = 1.0\n",
    "cond_seq[:, 1] = torch.FloatTensor(multi_linspace([(0,.3),(.5,1), (1,.3)], generation_length))\n",
    "#cond_seq[:, 2] = torch.linspace(0, 1, generation_length)\n",
    "cond_seq[:, 2] = torch.FloatTensor(steps(np.array([0,2,4,5,7,9,11,12])/12., generation_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973aecca-bd53-4dce-a3f6-b06147fb64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_condition_tensor(cond_seq, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54634131-0c27-4a04-bea3-ba8f567afdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_len = 32\n",
    "t = torch.linspace(0., 1., warmup_len)\n",
    "warmup_sequence = torch.sin(2 * np.pi * 220.0 * t)\n",
    "\n",
    "start_time = time.monotonic()\n",
    "generated_audio = run_inference(\n",
    "    model=model,\n",
    "    cond_seq=cond_seq,\n",
    "    warmup_sequence=warmup_sequence,\n",
    "    top_n=top_n,\n",
    "    temperature=temperature\n",
    ")\n",
    "elapsed_time = time.monotonic() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41137d-5014-4e49-a9f3-07e9fb9175ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9a68c3-de88-48b9-be3a-2ae8ac3e7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Saving waveform plot to {args.output_plot_path}\")\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.plot(generated_audio)\n",
    "plt.title(\"Generated Audio Waveform\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid()\n",
    "#plt.savefig(args.output_plot_path)\n",
    "#plt.close()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce1224-3131-4522-b72f-a1c9d6f4a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(generated_audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c5cdf-2da1-4ae4-b393-6fbc5e6e4405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
