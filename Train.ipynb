{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from audioDataLoader.mulaw import mu_law_encode, mu_law_decode\n",
    "from audioDataLoader.audio_dataset import AudioDatasetConfig, MuLawAudioDataset2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import model.gru_audio_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Params  \n",
    "<a id=\"dataparams\"></a>\n",
    "\n",
    "These parameters are pickled to file, and serve several purposes:\n",
    "* The allow other programs to properly evaluate and visualize the trained (and also saved) models,\n",
    "* PRovide a record of the parameters that allow reproducing results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_checkpoint= None # \"./output/20250805_162729/\"\n",
    "sourcedatadir='./data/nsynth.64.76.dl_new'\n",
    "\n",
    "params = dict(\n",
    "    # Read/write directory of data & parameter files\n",
    "    #*************************************\n",
    "    sample_rate=16000,\n",
    "    runTimeStamp='{:%Y-%m-%d_%H-%M-%S}'.format(datetime.now()),\n",
    "    \n",
    "    datadir = sourcedatadir,\n",
    "    paramdir = sourcedatadir,\n",
    "\n",
    "    savemodel = True,\n",
    "    savemodel_interval = 10, #in units of epochs\n",
    "    savemodeldir = os.getcwd() + '/output', #default saving directory for models and the parameterization\n",
    "    \n",
    "    batches_per_epoch = 100, #1000,  #10000, #max number batches of steps per epoch (typically num_epochs=1)\n",
    "    batch_size = 256, #256,\n",
    "    num_epochs = 10,\n",
    "\n",
    "    # Training parameters\n",
    "    #*************************************\n",
    "    noise=.1,\n",
    "    seqLen = 256, \n",
    "    stride = 1,\n",
    "    \n",
    "    lr = 0.005,\n",
    "    props={\"instID\": (1, 2), \"a\": (0,1), \"p\": (64.0, 76.0)},\n",
    "    \n",
    "    # Model parameters\n",
    "    hiddenSize = 48, #100,\n",
    "    nLayers = 4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = params['sample_rate']\n",
    "\n",
    "log_interval = 50 #log to tensorboard every n batches\n",
    "visualize_interval = log_interval\n",
    "\n",
    "#Generation parameters\n",
    "#*************************************\n",
    "max_length = params['seqLen']*3\n",
    "\n",
    "# Cuda\n",
    "#*************************************\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "#*************************************\n",
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "def mydate() :\n",
    "    return (datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "def inputcoding(samp) :\n",
    "    return mu_law_encode(np.array(samp))/255.\n",
    "\n",
    "def index2float(topi) :\n",
    "    return(mu_law_decode(topi[0][0]).cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--- model settings ----#\n",
    "model_config = model.gru_audio_model.GRUAudioConfig (\n",
    "    input_size = 1,\n",
    "    cond_size = len(params['props']),\n",
    "    hidden_size = params['hiddenSize'],\n",
    "    num_layers = params['nLayers'],\n",
    "    output_size = 256,  #mu-law quantization levels\n",
    "    dropout = 0.1\n",
    ")\n",
    "\n",
    "# ---- Training Settings ----\n",
    "data_config = AudioDatasetConfig(\n",
    "   data_dir=params['datadir'],\n",
    "   sequence_length=params['seqLen'],\n",
    "   parameter_specs=params['props'],\n",
    "   add_noise= False if params['noise'] == 0 else True,   # Whether to add white noise\n",
    "   noise_weight = params['noise'],                           # Desired signal-to-noise ratio (dB)\n",
    "   encode=True\n",
    ")\n",
    "\n",
    "# ---- Generation Settings ----\n",
    "testdata_config = AudioDatasetConfig(\n",
    "   data_dir=params['datadir'],\n",
    "   sequence_length=params['seqLen'],\n",
    "   parameter_specs=params['props'],\n",
    "   add_noise= False,                        # no noise for testing and priming\n",
    "   noise_weight = params['noise'],                           # Desired signal-to-noise ratio (dB)\n",
    "   encode=False\n",
    ")\n",
    "\n",
    "# === Dataset and Loader ===\n",
    "adataset = MuLawAudioDataset2(data_config)\n",
    "train_loader = DataLoader(adataset,\n",
    "                             batch_size=params['batch_size'],\n",
    "                             shuffle=True,\n",
    "                             num_workers=4,\n",
    "                             drop_last=True)\n",
    "\n",
    "testdataset = MuLawAudioDataset2(testdata_config)\n",
    "test_loader = DataLoader(testdataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4,\n",
    "                            drop_last=True)\n",
    "   \n",
    "\n",
    "print(\"size of dataset is\",len(adataset))\n",
    "print(\"no. of batches per epoch is\", params['batches_per_epoch'])\n",
    "print(\"batchsize id  is\", params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show stuff\n",
    "\n",
    "print(\"size of dataset is\",len(adataset))\n",
    "print(\"no. of batches per epoch is\", params['batches_per_epoch'])\n",
    "print(\"batchsize id  is\", params['batch_size'])\n",
    "\n",
    "samp = adataset.rand_sample()\n",
    "print(samp.shape)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp)), samp) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "samp = inputcoding(samp)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp)), samp) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "n_batches = 1\n",
    "print(\"Do it! {}\".format(mydate()))\n",
    "for bat, (x_audio_seq, y_target_seq) in enumerate(test_loader):\n",
    "    print(\"bat num {} at time {}\".format(bat, mydate()))\n",
    "    print(f\"x_audio_seq.shape = {x_audio_seq.shape}\") \n",
    "    print(f\"y_target_seq.shape = {y_target_seq.shape}\")\n",
    "    if bat >= n_batches :\n",
    "        break;\n",
    "print(\"finished at {}\".format(mydate()))\n",
    "\n",
    "# print(f\"x_audio_seq[0,:,0] seq is {x_audio_seq[0,:,0]}\")\n",
    "# print(f\"x_audio_seq[0,:,1] seq is {x_audio_seq[0,:,1]}\")\n",
    "# print(f\"x_audio_seq[0,:,2] seq is {x_audio_seq[0,:,2]}\")\n",
    "# print(f\"x_audio_seq[0,:,3] seq is {x_audio_seq[0,:,3]}\")\n",
    "# print(f\"y_target_seq[0,:] seq is {y_target_seq[0,:].squeeze(-1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Create Output Folders ----\n",
    "if resume_checkpoint != None:\n",
    "    out_dir = resume_checkpoint\n",
    "else:\n",
    "    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(params['savemodeldir'], run_timestamp)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{out_dir}/checkpoints\", exist_ok=True)\n",
    "    os.makedirs(f\"{out_dir}/tensorboard\", exist_ok=True)\n",
    "\n",
    "\n",
    "# ---- Save Config ----\n",
    "#machine readable\n",
    "torch.save({\n",
    "    \"model_config\": model_config,\n",
    "    \"data_config\": data_config\n",
    "}, f\"{out_dir}/config.pt\")\n",
    "\n",
    "#human readable\n",
    "with open(f\"{out_dir}/config.txt\", \"w\") as f:\n",
    "    f.write(\"model_config = \" + repr(model_config) + \"\\\\n\")\n",
    "    f.write(\"data_config = \" + repr(data_config) + \"\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training cycle (100% teacher forcing for now)\n",
    "#*************************************\n",
    "\n",
    "def train(model,epoch):\n",
    "\n",
    "\n",
    " model.train() #put in training mode\n",
    " ave_loss_over_steps = 0\n",
    "\n",
    " debug_seqs=0\n",
    "\n",
    " for batch_num, (inp, target) in enumerate(train_loader):\n",
    "\n",
    "     # if batch_num==0:\n",
    "     #     print(f\"train() - inp[:,:,0] - Min: {torch.min(inp[:,:,0])}, Max: {torch.max(inp[:,:,0])}, Average: {torch.mean(inp[:,:,0])}\")\n",
    "\n",
    "     debug_seqs=debug_seqs+len(inp) # summing the batch length for each batch_num\n",
    "\n",
    "     inp, target = inp.to(device), target.to(device)\n",
    "     # Forward + Backward + Optimize\n",
    "     hidden = model.init_hidden(params['batch_size'])\n",
    "     optimizer.zero_grad()\n",
    "     loss = 0\n",
    "\n",
    "     #print(f\" inp[0,127,0] (B,T,p) is {inp[0,127,0]}\")\n",
    "\n",
    "     # iterate through the SEQUENCE\n",
    "     for i in range(params['seqLen']):\n",
    "         outputs, hidden = model(inp[:,i,:],hidden,params['batch_size'])  #input dim: (batch, seq, feature)\n",
    "         loss += criterion(outputs, torch.squeeze(target[:,i],1))\n",
    "         #print(f\"timestep {i} target is {torch.squeeze(target[:,i],1)}\")\n",
    "\n",
    "     loss.backward()\n",
    "     optimizer.step()\n",
    "\n",
    "     ave_loss_per_sample = loss.item()/params['seqLen']   #over each minibatch\n",
    "     ave_loss_over_steps += ave_loss_per_sample\n",
    "\n",
    "     if (batch_num+1) % log_interval == 0:\n",
    "         print ('{:%Y-%m-%d %H:%M:%S} Epoch [{}/{}], batch_num [{}/{}] Loss: {:.4f}'.format(\n",
    "             datetime.now(), epoch+1, params['num_epochs'], batch_num+1, params['batches_per_epoch'], ave_loss_over_steps/log_interval))\n",
    "\n",
    "         list_of_losses.append(ave_loss_over_steps/log_interval)\n",
    "         writer.add_scalar(\"Loss/train\", ave_loss_per_sample, epoch)\n",
    "         ave_loss_over_steps = 0\n",
    "\n",
    "\n",
    "     if (batch_num+1) % visualize_interval == 0:\n",
    "         #result = generate(model,max_length)\n",
    "         result = newgen(model,max_length)\n",
    "         plt.figure(figsize=(20,1))\n",
    "         plt.plot(np.arange(len(result)), result) #just print one example from the batch\n",
    "         plt.show()\n",
    "         model.train() #put model back to training mode\n",
    "\n",
    "\n",
    "     if batch_num>=(params['batches_per_epoch']-1):\n",
    "         break\n",
    "\n",
    " print(f\"Finished epoch number {epoch} with a total of {debug_seqs} debug_seqs\")\n",
    " \n",
    " if (epoch + 1) % params['savemodel_interval'] == 0:\n",
    "     torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "     print(f\"Saved checkpoint at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for visualizing learning be showing one example of a generated sequence.\n",
    "# the model is warmed up with a random sequence from the data set and it's corresponding param vector\n",
    "from inference import run_inference\n",
    "\n",
    "def newgen(model,max_length):\n",
    "    p_inp, target = next(iter(test_loader))\n",
    "    x=p_inp[:,:,0].squeeze(0) # raw audio of length sequence_length\n",
    "    c=c_extended = p_inp[:,:,1:].squeeze(0).repeat_interleave(3, dim=0)  # Shape: [3*sequence_length, V]  \n",
    "    gen = run_inference(model, c, x, top_n=3, temperature=1.0)\n",
    "    return np.concatenate([x, gen]) #concatenated the warm up sequence with the genegenerated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#dataparams\">Go To Data Params</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn = model.gru_audio_model.RNN(model_config).to(device)\n",
    "    \n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=params['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start_epoch=0\n",
    "checkpoint_path = os.path.join(f\"{out_dir}/checkpoints\", \"last_checkpoint.pt\")\n",
    "if resume_checkpoint:\n",
    "    assert os.path.exists(checkpoint_path), f\"File {checkpoint_path} does not exist\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"{out_dir}/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_params = sum(p.numel() for p in rnn.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train!\n",
    "#*************************************\n",
    "list_of_losses = []\n",
    "\n",
    "print('{:%Y-%m-%d %H:%M:%S} Starting training...'.format(datetime.now()))\n",
    "start_time = time.monotonic()\n",
    "for epoch in range(start_epoch, start_epoch+params['num_epochs']):\n",
    "    train(rnn,epoch)\n",
    "writer.close()\n",
    "elapsed_time = time.monotonic() - start_time\n",
    "print('Training time taken:',time_taken(elapsed_time))\n",
    "\n",
    "\n",
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': rnn.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, checkpoint_path)\n",
    "print(f\"Saved checkpoint at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "#*************************************\n",
    "plt.figure()\n",
    "plt.plot(list_of_losses)\n",
    "plt.show()  # This will actually display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
