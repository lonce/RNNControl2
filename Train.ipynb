{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil # for copying checkpoint to \"last\"\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Audio\n",
    "\n",
    "from audioDataLoader.mulaw import mu_law_encode, mu_law_decode\n",
    "from audioDataLoader.audio_dataset import AudioDatasetConfig, MuLawAudioDataset2, MuLawAudioDatasetFromManifest\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import model.gru_audio_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Params  \n",
    "<a id=\"dataparams\"></a>\n",
    "\n",
    "These parameters are saved to file, and serve several purposes:\n",
    "* The allow other programs to properly evaluate and visualize the trained (and also saved) models,\n",
    "* Provide a record of the parameters that allow reproducing results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_checkpoint= None #str(Path(\"./output/20250817_151624_pistons_2048.16_4.96/\"))  # None # \n",
    "\n",
    "sourcedatadir = os.path.join(os.getcwd(), 'data', 'syn7mini', 'syn7mini_16k' )  # where the wav files live\n",
    "data_manifest = os.path.join(os.getcwd(), 'data', 'syn7mini', 'syn7applause_manifest.xlsx' )   # where the parameters for the files are found (as well as which files to select)\n",
    "savename=\"applause_modeltest\"  #just a tag on the output folder \n",
    "props={\"param_1\": (0,1)}\n",
    "#props={\"c1\": (0, 1), \"c2\": (0, 1),\"c3\": (0, 1),\"c4\": (0, 1),\"c5\": (0, 1),\"c6\": (0, 1),\"c7\": (0, 1),\"param_1\": (0,1)},\n",
    "n_q=1\n",
    "\n",
    "# ####   nsynth   ##########\n",
    "# sourcedatadir = os.path.join(os.getcwd(), 'data', 'nsynth.64.76_sm' )  # where the wav files live\n",
    "# data_manifest = os.path.join(os.getcwd(), 'data', 'nsynth.64.76_sm_manifest.xlsx' )   # where the parameters for the files are found (as well as which files to select)\n",
    "# savename=\"nsynth_modeltest\"  #just a tag on the output folder \n",
    "# props={\"instID\": (1, 2), \"a\": (0,1), \"p\": (64.0, 76.0)}\n",
    "# ##########################\n",
    "\n",
    "Datsetconfig=MuLawAudioDatasetFromManifest # MuLawAudioDataset2\n",
    "\n",
    "params = dict(\n",
    "    # Read/write directory of data & parameter files\n",
    "    #*************************************\n",
    "    sample_rate=16000,\n",
    "    runTimeStamp='{:%Y-%m-%d_%H-%M-%S}'.format(datetime.now()),\n",
    "    \n",
    "    datadir = sourcedatadir,\n",
    "    data_manifest = data_manifest,\n",
    "    paramdir = sourcedatadir,\n",
    "    \n",
    "    savemodel = True,\n",
    "    savemodel_interval = 10, # in units of epochs\n",
    "    savemodeldir = os.path.join(os.getcwd(), 'output'), # default saving directory for models and the parameterization\n",
    "\n",
    "    # Training parameters ----------------------------------------------\n",
    "    num_epochs = 20, # of batches_per_epoch of batch_size sequeunce\n",
    "    batches_per_epoch = 100, \n",
    "    batch_size = 100, \n",
    "    \n",
    "    noise=.1,\n",
    "    seqLen = 256,\n",
    "    #stride = 1,\n",
    "    \n",
    "    lr = 0.005,\n",
    "    # parameter names for the Data Loader to search for in the filenames, and the range that maps to [0,1] for training and inference\n",
    "    # The order specified in this structure will be the order used for inference\n",
    "    #props={\"instID\": (1, 2), \"a\": (0,1), \"p\": (64.0, 76.0)},\n",
    "    #props={\"c1\": (0, 1), \"c2\": (0, 1),\"c3\": (0, 1),\"c4\": (0, 1),\"c5\": (0, 1),\"c6\": (0, 1),\"c7\": (0, 1),\"param_1\": (0,1)},\n",
    "    props=props,\n",
    "    \n",
    "    # Model parameters\n",
    "    input_size = 1,\n",
    "    hiddenSize = 48, #100,\n",
    "    nLayers = 4,\n",
    "    codebook_size = 128, # mulaw quant levels\n",
    "    dropout = 0.1,\n",
    "    n_q = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = params['sample_rate']\n",
    "\n",
    "log_interval = 1 # units of epochs\n",
    "visualize_interval = log_interval # units of epochs\n",
    "\n",
    "#Generation parameters\n",
    "#*************************************\n",
    "max_length = params['seqLen']*3  #length of the sequence used to inspect progress in audio plots during training\n",
    "\n",
    "# Cuda\n",
    "#*************************************\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#device='cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utility functions\n",
    "#*************************************\n",
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "def mydate() :\n",
    "    return (datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "def inputcoding(samp) :\n",
    "    return mu_law_encode(np.array(samp), params['codebook_size'])/(params['codebook_size']-1.0)\n",
    "\n",
    "def index2float(topi) :\n",
    "    return(mu_law_decode(topi[0][0]).cpu().numpy(), params['codebook_size'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#--- model settings ----#\n",
    "model_config = model.gru_audio_model.GRUModelConfig (\n",
    "    input_size = params['input_size'],\n",
    "    cond_size = len(params['props']),\n",
    "    hidden_size = params['hiddenSize'],\n",
    "    num_layers = params['nLayers'],\n",
    "    codebook_size = params['codebook_size'],  #mu-law quantization levels\n",
    "    dropout = params['dropout'],\n",
    "    n_q=params['n_q']\n",
    ")\n",
    "\n",
    "# ---- Training Settings ----\n",
    "data_config = AudioDatasetConfig(\n",
    "   data_dir=params['datadir'],\n",
    "   data_manifest = params['data_manifest'],\n",
    "   sequence_length=params['seqLen'],\n",
    "   parameter_specs=params['props'],\n",
    "   add_noise= False if params['noise'] == 0 else True,   # Whether to add white noise\n",
    "   noise_weight = params['noise'],                           # Desired signal-to-noise ratio (dB)\n",
    "   encode=True,\n",
    "   codebook_size=params['codebook_size']\n",
    ")\n",
    "\n",
    "# ---- Generation Settings ----\n",
    "testdata_config = AudioDatasetConfig(\n",
    "   data_dir=params['datadir'],\n",
    "   data_manifest = params['data_manifest'],\n",
    "   sequence_length=params['seqLen'],\n",
    "   parameter_specs=params['props'],\n",
    "   add_noise= False,                        # no noise for testing and priming\n",
    "   noise_weight = params['noise'],                           # Desired signal-to-noise ratio (dB)\n",
    "   encode=False,\n",
    "   codebook_size=params['codebook_size']\n",
    ")\n",
    "\n",
    "# === Dataset and Loader ===\n",
    "adataset = Datsetconfig(data_config)\n",
    "train_loader = DataLoader(adataset,\n",
    "                             batch_size=params['batch_size'],\n",
    "                             shuffle=True,\n",
    "                             num_workers=4,\n",
    "                             drop_last=True)\n",
    "\n",
    "testdataset = Datsetconfig(testdata_config)\n",
    "test_loader = DataLoader(testdataset,\n",
    "                            batch_size=1,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4,\n",
    "                            drop_last=True)\n",
    "   \n",
    "# The data \"size\" is the number of possible sequences the data loader can provide.\n",
    "# Since the sequences are chosen randomly from the file, the number of possible sequence is the total number of audio samples in the set (minus the ones that start less than sequence length from the end of the files)\n",
    "print(\"size of dataset is\",len(adataset))\n",
    "print(\"no. of batches per epoch is\", params['batches_per_epoch'])\n",
    "print(\"batchsize id  is\", params['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just show some stuff \n",
    "\n",
    "print(\"size of dataset is\",len(adataset))\n",
    "print(\"no. of batches per epoch is\", params['batches_per_epoch'])\n",
    "print(\"batchsize id  is\", params['batch_size'])\n",
    "\n",
    "samp, pvect = adataset.rand_sample()\n",
    "print(f\"pvect: {pvect}\")\n",
    "print(samp.shape)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(samp)), samp) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "msamp = inputcoding(samp)\n",
    "plt.figure(figsize=(20,1)) \n",
    "plt.plot(np.arange(len(msamp)), msamp) #just print one example from the batch\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "n_batches = 1\n",
    "for bat, (x_audio_seq, y_target_seq) in enumerate(test_loader):\n",
    "    print(\"bat num {} at time {}\".format(bat, mydate()))\n",
    "    print(f\"x_audio_seq.shape = {x_audio_seq.shape}\") \n",
    "    print(f\"y_target_seq.shape = {y_target_seq.shape}\")\n",
    "    print(f\"conditioning params: {x_audio_seq[0,0:3,1:]}\")\n",
    "    if bat >= n_batches :\n",
    "        break;\n",
    "print(\"finished at {}\".format(mydate()))\n",
    "\n",
    "#This is the \"rand_samp\" plotted above\"\n",
    "Audio(samp, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training cycle (100% teacher forcing for now)\n",
    "#*************************************\n",
    "\n",
    "def train(model,epoch):\n",
    "\n",
    " model.train() #put in training mode\n",
    " ave_loss_over_steps = 0\n",
    " debug_seqs=0\n",
    "\n",
    " for batch_num, (inp, target) in enumerate(train_loader):\n",
    "     # if batch_num==0:\n",
    "     #     print(f\"train() - inp[:,:,0] - Min: {torch.min(inp[:,:,0])}, Max: {torch.max(inp[:,:,0])}, Average: {torch.mean(inp[:,:,0])}\")\n",
    "\n",
    "     debug_seqs=debug_seqs+len(inp) # summing the batch length for each batch_num\n",
    "\n",
    "     inp, target = inp.to(device), target.to(device)\n",
    "     # Forward + Backward + Optimize\n",
    "     hidden = model.init_hidden(params['batch_size'])\n",
    "     optimizer.zero_grad()\n",
    "     loss = 0\n",
    "\n",
    "     #print(f\" inp[0,127,0] (B,T,p) is {inp[0,127,0]}\")\n",
    "\n",
    "     # # iterate through the SEQUENCE, one single sequence step at a time\n",
    "     # for i in range(params['seqLen']):\n",
    "     #     outputs, hidden = model(inp[:,i,:],hidden,params['batch_size'])  #input dim: (batch, seq, feature)\n",
    "     #     for j in range(params[n_q])\n",
    "     #         loss += criterion(outputs[j], torch.squeeze(target[:,i],1))\n",
    "     #     #print(f\"timestep {i} target is {torch.squeeze(target[:,i],1)}\")\n",
    "\n",
    "     for i in range(params['seqLen']):\n",
    "        outputs, hidden = model(inp[:,i,:], hidden, params['batch_size'])\n",
    "        for j in range(params['n_q']):\n",
    "            loss += criterion(outputs[j], torch.squeeze(target[:,i,j], 1))\n",
    "\n",
    "        \n",
    "     loss.backward()\n",
    "     optimizer.step()\n",
    "\n",
    "     ave_loss_per_sample = loss.item()/params['seqLen']   #over each minibatch\n",
    "     ave_loss_over_steps += ave_loss_per_sample\n",
    "\n",
    "     if batch_num>=(params['batches_per_epoch']-1):\n",
    "         break\n",
    "\n",
    " print(f\"Finished epoch number {epoch} with a total of {debug_seqs} debug_seqs\")\n",
    "\n",
    "\n",
    " if (epoch+1) % log_interval == 0:\n",
    "     print(f\" time: {datetime.now()}, epoch {epoch+1},  Loss: {ave_loss_per_sample:.4f}\")\n",
    "     list_of_losses.append(ave_loss_per_sample)\n",
    "     writer.add_scalar(\"Loss/train\", ave_loss_per_sample, epoch+1)\n",
    "\n",
    " if (epoch+1) % visualize_interval == 0:\n",
    "\n",
    "     # these two lines address a resource management issues with temporary directories ...  I think....\n",
    "     torch.cuda.empty_cache()  # if using GPU\n",
    "     gc.collect()\n",
    "\n",
    "     result = newgen(model,max_length)\n",
    "     plt.figure(figsize=(20,1))\n",
    "     plt.plot(np.arange(len(result)), result) #just print one example from the batch\n",
    "     plt.show()\n",
    "     model.train() #put model back to training mode\n",
    "\n",
    "\n",
    " # overwrite the last, and save a numbered checkpoint\n",
    " if (epoch + 1) % params['savemodel_interval'] == 0:\n",
    "    checkpoint_data = {\n",
    "        'epoch': epoch+1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }\n",
    "    \n",
    "    # Save numbered checkpoint\n",
    "    numbered_path = os.path.join(f\"{out_dir}/checkpoints\", f\"checkpoint_{epoch+1}.pt\")\n",
    "    torch.save(checkpoint_data, numbered_path)\n",
    "    \n",
    "    # Copy to \"last\" (much faster than saving twice)\n",
    "    last_path = os.path.join(f\"{out_dir}/checkpoints\", \"last_checkpoint.pt\")\n",
    "    shutil.copy2(numbered_path, last_path)\n",
    "    \n",
    "    print(f\"Saved checkpoint at epoch {epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Create Output Folders ----\n",
    "if resume_checkpoint != None:\n",
    "    out_dir = resume_checkpoint\n",
    "else:\n",
    "    run_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    out_dir = os.path.join(params['savemodeldir'], run_timestamp + \"_\"+savename)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{out_dir}/checkpoints\", exist_ok=True)\n",
    "    os.makedirs(f\"{out_dir}/tensorboard\", exist_ok=True)\n",
    "\n",
    "\n",
    "# ---- Save Config ----\n",
    "#machine readable\n",
    "torch.save({\n",
    "    \"model_config\": model_config,\n",
    "    \"data_config\": data_config\n",
    "}, f\"{out_dir}/config.pt\")\n",
    "\n",
    "#human readable\n",
    "with open(f\"{out_dir}/config.txt\", \"w\") as f: \n",
    "    f.write(\"params = \" + repr(params) + \"\\\\n\")\n",
    "    f.write(\"model_config = \" + repr(model_config) + \"\\\\n\")\n",
    "    f.write(\"data_config = \" + repr(data_config) + \"\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for visualizing learning be showing one example of a generated sequence.\n",
    "# the model is warmed up with a random sequence from the data set and it's corresponding param vector\n",
    "from inference import run_inference\n",
    "\n",
    "def newgen(model,max_length):\n",
    "    p_inp, _ = next(iter(test_loader))\n",
    "    x=p_inp[:,:,0].squeeze(0) # raw audio of length sequence_length\n",
    "    c=c_extended = p_inp[:,:,1:].squeeze(0).repeat_interleave(3, dim=0)  # Shape: [3*sequence_length, V]  \n",
    "    gen = run_inference(model, c, x, top_n=3, temperature=1.0)\n",
    "    return np.concatenate([x, gen]) #concatenated the warm up sequence with the genegenerated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn = model.gru_audio_model.RNN(model_config).to(device)\n",
    "    \n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=params['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start_epoch=0\n",
    "\n",
    "if resume_checkpoint:\n",
    "    checkpoint_path = os.path.join(f\"{out_dir}/checkpoints\", \"last_checkpoint.pt\")\n",
    "    assert os.path.exists(checkpoint_path), f\"File {checkpoint_path} does not exist\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resumed from checkpoint at epoch {start_epoch}\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"{out_dir}/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trainable_params = sum(p.numel() for p in rnn.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "#*************************************\n",
    "list_of_losses = []\n",
    "\n",
    "print('{:%Y-%m-%d %H:%M:%S} Starting training...'.format(datetime.now()))\n",
    "start_time = time.monotonic()\n",
    "#-------------------------------------------------------------------------------\n",
    "for epoch in range(start_epoch, start_epoch+params['num_epochs']):\n",
    "    train(rnn,epoch)\n",
    "#-------------------------------------------------------------------------------\n",
    "writer.close()\n",
    "elapsed_time = time.monotonic() - start_time\n",
    "print('Training time taken:',time_taken(elapsed_time))\n",
    "\n",
    "#Just make sure the final model gets saved\n",
    "checkpoint_data = {\n",
    "    'epoch': epoch+1,\n",
    "    'model_state_dict': rnn.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict()\n",
    "}\n",
    "# Save numbered checkpoint\n",
    "numbered_path = os.path.join(f\"{out_dir}/checkpoints\", f\"checkpoint_{epoch+1}.pt\")\n",
    "torch.save(checkpoint_data, numbered_path)\n",
    "print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "# Copy to \"last\" (much faster than saving twice)\n",
    "last_path = os.path.join(f\"{out_dir}/checkpoints\", \"last_checkpoint.pt\")\n",
    "shutil.copy2(numbered_path, last_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "#*************************************\n",
    "plt.figure()\n",
    "plt.plot(list_of_losses)\n",
    "plt.show()  # This will actually display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
